# Simple Scraper Configuration
#

# Log file - If not specified log messages will be sent to stdout
log_file = "squeegee.log"

# The number of concurrent HTTP fetches performed
# Please be nice, obey robots.txt, and don't overload someone's server
num_concurrent = 40

# The number of errors to allow for a proxy before removing it
num_err_remove_proxy = 5

# The number of errors to allow for a URL before removing it
num_err_remove_url = 5

# The cache file to be used for subsequent runs.  No cache if not present
cache_file = "squeegee.db"

# The file name of the list of proxies.  No proxying will be done
# if not specified.  Proxies are randomized per fetch and are specified in
# the file, one per line, in the formats described below:
# www.example.com   # assume port 80
# 123.123.123.123
# www.example2.com:8080
# 123.123.123.132:8080
# username:password@www.example3.com:8080
# etc.
proxyListFile = ""

# The URLs from which to begin the scrape.  These URLs need not be from the
# same host as we can have multiple outputs.
# Not that further URLs can be scraped from the pages themselves (specified below)
scrape_urls = [
    "http://www.proxies.by/raw_free_db.htm?0",
]

# The list of useragents to be randomized, one per fetch.  Below we're simply
# using the GoogleBot's useragent string to discourage javascript usage.
useragents = [
    "Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)",
]

# We're only scraping a single page type here, otherwise we would specify
# multiple [[match_urls]] sections in this config file.
[[match_urls]]
    url_regex = "www.proxies.by/raw_free_db.htm"
    [[match_urls.find_url_patterns]]
        parser_type = "regex"
        parser_data = ['href="(\?[0-9]+)"', "0"]
        # Should we log if we don't find the regex or xpath on the page?
        log_if_unfound = false
        # Should we stop if we don't find the regex or xpath on the page?
        stop_if_unfound = false
   [[match_urls.outputs]]
        # What's output file and type (specified by extension) (required)
        file_name = "proxies.txt"
        # What are the column names (only one column allowed for a txt file output)
        columns = ["proxy"]
        # Should the column names be used in the output.  (disregarded for structured output like JSON)
        include_column_names = false
        # Do we output a row even if we don't have all the columns.  Not used for our case.
        require_all_columns = true
        # Should we log if we don't find at least one valid output on the page?
        log_if_unfound = false
        # Should we stop if we don't find at least one valid output on the page?
        stop_if_unfound = false
        [[match_urls.outputs.patterns]]
            # This match will fill the proxy column
            column = "proxy"
            parser_type = "regex"
            parser_data = [
                '> *(([0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}(:[0-9]{4,5}){0,1})|([a-z0-9]+\.[a-z0-9]+\.[a-z0-9]+(\.[a-z0-9]+){0,1}(:[0-9]{4,5}){0,1})).*?<td *> *HTTP',
                "1",]
